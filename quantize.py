import os
import argparse
import sys
import timeit
from prettytable import PrettyTable
from joblib import load
from sklearn.preprocessing import StandardScaler

# Silence TensorFlow messages
os.environ['TF_CPP_MIN_LOG_LEVEL']='3'

import tensorflow as tf
import pandas as pd
import h5py
import numpy as np

from tensorflow.keras.layers import Dropout
from tensorflow_model_optimization.quantization.keras import vitis_quantize

ap = argparse.ArgumentParser()
ap.add_argument('-m', '--float_model', type=str, default='inputFiles/best_model.h5', help='Path of floating-point model. Default is inputFiles/best_model.h5')
ap.add_argument('-q', '--quant_model', type=str, default='build/quant_model/quantized_model.h5', help='Path of quantized model. Default is build/quant_model/quantized_model.h5')
ap.add_argument('-b', '--batchsize',   type=int, default=50, help='Batchsize for quantization. Default is 50')
ap.add_argument('-v', '--verbose', action='store_true', help='Enable verbose mode')
args = ap.parse_args()

############### Print of info
print ('\n------------------------------------')
print ('TensorFlow version : ',tf.__version__)
print ('System : ', sys.version)
print ('------------------------------------')
print ('Command line options:')
print (' --float_model  : ', args.float_model)
print (' --quant_model  : ', args.quant_model)
print (' --batchsize    : ', args.batchsize)
print ('------------------------------------\n')

############### Factors we want to consider during quantization
### - model accuracy: the most fundamental evaluaiton of a NN. 
### - Model size: readuing the model's size can make it more suitable for an FPGA
### - Latency: latency measures the time it takes for a model to make a prediciton. Lower latency is generally better, especailly for real-time applications
### - Throughput: similar as latency, but focuses on overall processing capability. time for each predicitons
### - Power efficiency: this is important for embedded applicaitons (but typically measured by running the model on the target FPGA, we cannot do it here.)
### - Resource utilization: this measures how much of the FPGA's resources (e.g., logic elements, DSP blocks, memory) the model uses. This should be available in the report generated by Vitis AI complier
###### so in the following, creating a pretty table to show some of the facotrs for the original and quantized model for comparison

with h5py.File('inputFiles/subset_df_test.h5', 'r') as h5f:
	labels = np.array(h5f['labels']) if 'labels' in h5f else None
	input_data = {name: np.array(data) for name, data in h5f.items() if name != 'labels'}

if args.verbose:
	print("===========================================")
	print("For data directly loaded from hdf5 file:")
	for col_name, data_array in input_data.items():
		print(f"-- Feature: {col_name}, Data Type: {data_array.dtype}")

scaler = load('inputFiles/std_scaler.bin')
input_df = pd.DataFrame(input_data)
scaled_array = scaler.transform(input_df)

if args.verbose:
	print("===========================================")
	print("After scaler:")
	scaled_data = {col_name: scaled_array[:, idx] for idx, col_name in enumerate(input_df.columns)}
	for col_name, data_array in scaled_data.items():
		print(f"-- Feature: {col_name}, Data Type: {data_array.dtype}")

## Quantize data
scaled_data_quant = scaled_array
#scaled_data_quant = scaled_array.astype(np.float16)
if args.verbose:
	print("===========================================")
	print("After quantizing the input:")
	tmp_scaled_data_quant = {col_name: scaled_data_quant[:, idx] for idx, col_name in enumerate(input_df.columns)}
	for col_name, data_array in tmp_scaled_data_quant.items():
		print(f"-- Feature: {col_name}, Data Type: {data_array.dtype}")

## load trained model
model = tf.keras.models.load_model(args.float_model)
orig_model =  model
## compile the original model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
if args.verbose:
	print("===========================================")
	print("Original model summary:")
	for layer in model.layers:
		print(f"Layer Name: {layer.name}")
		if layer.get_weights():
			weights = layer.get_weights()[0]
			print(f" - Weight Shape: {weights.shape}")
			print(f" - Weight Type: {weights.dtype}")
			#print(f" - Weight Values: {weights}")

## Evaluate the orignal model
print("\nEvaluating Original Model...")
orig_start_time = timeit.default_timer()
orig_loss, orig_acc = model.evaluate(scaled_array, labels)
orig_final_time = timeit.default_timer()
orig_latency = orig_final_time - orig_start_time
orig_throughput = len(labels) / orig_latency
orig_model_size = os.path.getsize(args.float_model) / 1024.

## Applying Quantization using Vitis Quantizer
quantizer = vitis_quantize.VitisQuantizer(orig_model)
quantized_model = quantizer.quantize_model(calib_dataset=scaled_data_quant)
## Compile and retrain the model
quantized_model.save('build/quant_model/quantized_model.h5')
if args.verbose:
	print("===========================================")
	print("Quantized model summary:")
	for layer in quantized_model.layers:
		print(f"Layer Name: {layer.name}")
		if layer.get_weights():
			weights = layer.get_weights()[0]
			print(f" - Weight Shape: {weights.shape}")
			print(f" - Weight Type: {weights.dtype}")
			#print(f" - Weight Values: {weights}")

# Evaluate the Quantized Model with quantized features
print("\nEvaluating Quantized Model...")
quantized_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
quant_start_time = timeit.default_timer()
quant_loss, quant_acc = quantized_model.evaluate(scaled_data_quant, labels)
quant_final_time = timeit.default_timer()
quant_latency = quant_final_time - quant_start_time
quant_throughput = len(labels) / quant_latency
quant_model_size = os.path.getsize('build/quant_model/quantized_model.h5') / 1024.

## Just for test:
#converter = tf.lite.TFLiteConverter.from_keras_model(quantized_model)
#converter.optimizations = [tf.lite.Optimize.DEFAULT]
#quantized_tflite_model = converter.convert()
#if args.verbose:
#	print("\nQuantized model summary:")
#	interpreter = tf.lite.Interpreter(model_content=quantized_tflite_model)
#	interpreter.allocate_tensors()
#	for tensor_details in interpreter.get_tensor_details():
#		print(f"Layer Name: {tensor_details['name']}")
#		print(f" - Weight Shape: {tensor_details['shape']}")
#		print(f" - Weight Type: {tensor_details['dtype']}")
#print("\nEvaluating Fully Quantized Model...")
#fully_quant_start_time = timeit.default_timer()
#fully_quant_loss, fully_quant_acc, fully_quant_auc = quantized_model.evaluate(scaled_data_quant, labels, 32)
#fully_quant_final_time = timeit.default_timer()
#fully_quant_latency = fully_quant_final_time - fully_quant_start_time
#fully_quant_throughput = len(labels) / fully_quant_latency

## Retrain the model after quantization (not sure if we want this so far, so commeting it out)
## Not recommended here, using testing dataset to re-train.
## TO-DO: will have to ask for the training dataset and update the sample here.
#quantized_model.fit(scaled_data_quant, labels, batch_size=args.batchsize, epochs=5)
#quantized_model.save('build/quant_model/quantized_retrained_model.h5')
#retrain_quant_start_time = timeit.default_timer()
#retrain_quant_loss, retrain_quant_acc, retrain_quant_auc = quantized_model.evaluate(scaled_data_quant, labels, 32)
#retrain_quant_final_time = timeit.default_timer()
#retrain_quant_latency = retrain_quant_final_time - retrain_quant_start_time
#retrain_quant_throughput = len(labels) / retrain_quant_latency
#retrain_quant_model_size = os.path.getsize('build/quant_model/quantized_retrained_model.h5') / 1024.

# Compare original model performance with quantized model
print("\nSummarizing the performance between the orignal and quantized models:")
table = PrettyTable()
table.field_names = ["Factor", "Original Model", "Quantized Model"]
table.add_row(["Accuracy[%]", format(orig_acc*100, '.2f'), format(quant_acc*100, '.2f')])
table.add_row(["Size (KB)", format(orig_model_size, '.2f'), format(quant_model_size, '.2f')])
table.add_row(["Latency (s)", format(orig_latency, '.2f'), format(quant_latency, '.2f')])
table.add_row(["Throughput", format(orig_throughput, '.2f'), format(quant_throughput,'.2f')])
print(table)
