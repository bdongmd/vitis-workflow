import os
import argparse
import sys
import timeit
from prettytable import PrettyTable
from joblib import load
from sklearn.preprocessing import StandardScaler

# Silence TensorFlow messages
os.environ['TF_CPP_MIN_LOG_LEVEL']='3'

import tensorflow as tf
import pandas as pd
import h5py
import numpy as np

from tensorflow.keras.layers import Dropout
from tensorflow_model_optimization.quantization.keras import vitis_quantize
from tensorflow_model_optimization.quantization.keras import vitis_inspect

ap = argparse.ArgumentParser()
ap.add_argument('-m', '--float_model', type=str, default='inputFiles/best_model.h5', help='Path of floating-point model. Default is inputFiles/best_model.h5')
ap.add_argument('-q', '--quant_model', type=str, default='build/quant_model/best_q_model.h5', help='Path of quantized model. Default is build/quant_model/best_q_model.h5')
ap.add_argument('-b', '--batchsize',   type=int, default=50, help='Batchsize for quantization. Default is 50')
args = ap.parse_args()

############### Print of info
print('\n------------------------------------')
print('TensorFlow version : ',tf.__version__)
print(sys.version)
print('------------------------------------')
print ('Command line options:')
print (' --float_model  : ', args.float_model)
print (' --quant_model  : ', args.quant_model)
print (' --batchsize    : ', args.batchsize)
print('------------------------------------\n')

############### Factors we want to consider during quantization
### - model accuracy: the most fundamental evaluaiton of a NN. 
### - Model size: readuing the model's size can make it more suitable for an FPGA
### - Latency: latency measures the time it takes for a model to make a prediciton. Lower latency is generally better, especailly for real-time applications
### - Throughput: similar as latency, but focuses on overall processing capability. time for each predicitons
### - Power efficiency: this is important for embedded applicaitons (but typically measured by running the model on the target FPGA, we cannot do it here.)
### - Resource utilization: this measures how much of the FPGA's resources (e.g., logic elements, DSP blocks, memory) the model uses. This should be available in the report generated by Vitis AI complier
###### so in the following, creating a pretty table to show some of the facotrs for the original and quantized model for comparison
table = PrettyTable()
table.field_names = ["Factor", "Original Model", "Quantized Model"]

# ---- SPECIFY QUANTIZATIONS AND CREATE ASSOCIATED STRING -----

quant_dict = {'e_T':np.float16,'dR':np.int8,'weight':np.float16}
quant_array = np.empty(3)
for key in quant_dict.keys():
    if key == 'e_T':
        index = 0
    elif key == 'dR':
        index = 1    
    elif key == 'weight':
        index = 2

    if quant_dict[key] == np.int8:
        quant_array[index] = '8' 
    if quant_dict[key] == np.float16:
        quant_array[index] = '16' 
    if quant_dict[key] == np.float32:
        quant_array[index] = '32' 
    if quant_dict[key] == np.float64:
        quant_array[index] = '64' 

quant_string = ''
for quant in quant_array:
    quant_string += str(int(quant))+'-'
quant_string = quant_string[:-1]

# ------- IMPORT DATA, QUANTIZE FEATURES, AND SCALE ---------

h5f = h5py.File('inputFiles/df_test.h5', 'r')
scaler = load('inputFiles/std_scaler.bin')

col_names = []
with open('inputFiles/train_var_list.txt','r') as train_var: 
    for line in train_var:
        col_names.append(line.strip())

features = pd.DataFrame(h5f['X_test'][:], columns=col_names)
labels = np.array(h5f['Y_test'], dtype=np.int64)

quant_features = features.copy()
for i, name in enumerate(col_names):
    if i % 2 == 1:
        quant_features[name] = quant_features[name].astype(quant_dict['e_T'])
    else:
        quant_features[name] = quant_features[name].astype(quant_dict['dR'])

print('\n\n ----- INPUT INFO ----- \n')
print('features head: ')
print(features[col_names[:10]].head())
print('features.iloc[0,1,2].dtype:'+str(features.iloc[0].dtype)+', '+str(features.iloc[1].dtype)+', '+str(features.iloc[2].dtype))

print('quantized features head: ')
print(quant_features[col_names[:10]].head())
print('quantized features.iloc[0,1,2].dtype:'+str(quant_features.iloc[0].dtype)+', '+str(quant_features.iloc[1].dtype)+', '+str(quant_features.iloc[2].dtype))

features = scaler.transform(features)
quant_features = scaler.transform(quant_features)
labels = np.argmax(labels, axis=-1)
h5f.close()

## load trained model
model = tf.keras.models.load_model(args.float_model)
print(model.summary())

## compile the original model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.AUC(name='auc')])

## Evaluate the orignal model
print("\nEvaluating Original Model...")
orig_start_time = timeit.default_timer()
orig_loss, orig_acc, orig_auc = model.evaluate(features, labels, 32)
orig_final_time = timeit.default_timer()
orig_latency = orig_final_time - orig_start_time
orig_throughput = len(labels) / orig_latency
orig_model_size = os.path.getsize(args.float_model) / 1024.

## Applying Quantization using Vitis Quantizer
quantizer = vitis_quantize.VitisQuantizer(model)
quantized_model = quantizer.quantize_model(calib_dataset=features)

# Perform model inspections -- doesn't work

#vitis_inspect.VitisInspector('idk_what_this_does_lets_see').inspect_model(model, 
#                             input_shape=None,
#                             dump_model=True,
#                             dump_model_file="og_model.h5",
#                             plot=True,
#                             plot_file="og_model.svg",
#                             dump_results=True,
#                             dump_results_file="og_inspect_results.txt",
#                             verbose=0)
#
#vitis_inspect.VitisInspector('idk_what_this_does_lets_see2').inspect_model(quantized_model, 
#                             input_shape=None,
#                             dump_model=True,
#                             dump_model_file="quant_model.h5",
#                             plot=True,
#                             plot_file="quant_model.svg",
#                             dump_results=True,
#                             dump_results_file="quant_inspect_results.txt",
#                             verbose=0)

## Compile and retrain the model
quantized_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.AUC(name='auc')])
## Retrain the model after quantization (not sure if we want this so far, so commeting it out)
## quantized_model.fit(features_quant, labels, batch_size=args.batchsize, epochs=5)
quantized_model.save('../output/quantized_model.h5')

# Evaluate the Quantized Model with quantized features
print("\nEvaluating Quantized Model...")
quant_start_time = timeit.default_timer()
quant_loss, quant_acc, quant_auc = quantized_model.evaluate(quant_features, labels)
quant_final_time = timeit.default_timer()
quant_latency = quant_final_time - quant_start_time
quant_throughput = len(labels) / quant_latency
quant_model_size = os.path.getsize('../output/quantized_model.h5') / 1024.

# Compare original model performance with quantized model
print("\nSummarizing the performance between the orignal and quantized models:")
table.add_row(["Accuracy", format(orig_acc, '.4f'), format(quant_acc, '.4f')])
table.add_row(["AUC", format(orig_auc, '.4f'), format(quant_auc, '.4f')])
table.add_row(["Size (KB)", format(orig_model_size, '.2f'), format(quant_model_size, '.2f')])
table.add_row(["Latency (s)", format(orig_latency, '.2f'), format(quant_latency, '.2f')])
table.add_row(["Throughput", format(orig_throughput, '.2f'), format(quant_throughput,'.2f')])
table_string = table.get_string()
print(table)
with open("../output/prettyTable"+quant_string+".txt", "w") as file:
    file.write(table_string)