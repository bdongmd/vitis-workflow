import os
import argparse
import sys
import timeit
from prettytable import PrettyTable

# Silence TensorFlow messages
os.environ['TF_CPP_MIN_LOG_LEVEL']='3'

import tensorflow as tf
import pandas as pd
import h5py
import numpy as np

from tensorflow.keras.layers import Dropout
from tensorflow_model_optimization.quantization.keras import vitis_quantize

ap = argparse.ArgumentParser()
ap.add_argument('-m', '--float_model', type=str, default='inputFiles/best_model.h5', help='Path of floating-point model. Default is inputFiles/best_model.h5')
ap.add_argument('-q', '--quant_model', type=str, default='build/quant_model/best_q_model.h5', help='Path of quantized model. Default is build/quant_model/best_q_model.h5')
ap.add_argument('-b', '--batchsize',   type=int, default=50, help='Batchsize for quantization. Default is 50')
args = ap.parse_args()

############### Print of info
print('\n------------------------------------')
print('TensorFlow version : ',tf.__version__)
print(sys.version)
print('------------------------------------')
print ('Command line options:')
print (' --float_model  : ', args.float_model)
print (' --quant_model  : ', args.quant_model)
print (' --batchsize    : ', args.batchsize)
print('------------------------------------\n')

############### Factors we want to consider during quantization
### - model accuracy: the most fundamental evaluaiton of a NN. 
### - Model size: readuing the model's size can make it more suitable for an FPGA
### - Latency: latency measures the time it takes for a model to make a prediciton. Lower latency is generally better, especailly for real-time applications
### - Throughput: similar as latency, but focuses on overall processing capability. time for each predicitons
### - Power efficiency: this is important for embedded applicaitons (but typically measured by running the model on the target FPGA, we cannot do it here.)
### - Resource utilization: this measures how much of the FPGA's resources (e.g., logic elements, DSP blocks, memory) the model uses. This should be available in the report generated by Vitis AI complier
###### so in the following, creating a pretty table to show some of the facotrs for the original and quantized model for comparison
table = PrettyTable()
table.field_names = ["Factor", "Original Model", "Quantized Model"]

h5f = h5py.File('inputFiles/df_test.h5', 'r')
features = np.array(h5f['X_test'], dtype=np.float32)
labels = np.array(h5f['Y_test'], dtype=np.int64)
labels = np.argmax(labels, axis=-1)
h5f.close()

## load trained model
model = tf.keras.models.load_model(args.float_model)
print(model.summary())

## compile the original model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.AUC(name='auc')])

## Evaluate the orignal model
print("\nEvaluating Original Model...")
orig_start_time = timeit.default_timer()
orig_loss, orig_acc, orig_auc = model.evaluate(features, labels, 32)
orig_final_time = timeit.default_timer()
orig_latency = orig_final_time - orig_start_time
orig_throughput = len(labels) / orig_latency
orig_model_size = os.path.getsize(args.float_model) / 1024.

## we want to quantize everything
## first here goes quantizing feature
features_quant = features.astype(np.float16)

## Applying Quantization using Vitis Quantizer
quantizer = vitis_quantize.VitisQuantizer(model)
quantized_model = quantizer.quantize_model(calib_dataset=features_quant)

## Compile and retrain the model
quantized_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.AUC(name='auc')])
## Retrain the model after quantization (not sure if we want this so far, so commeting it out)
## quantized_model.fit(features_quant, labels, batch_size=args.batchsize, epochs=5)
quantized_model.save('../output/quantized_model.h5')

# Evaluate the Quantized Model with quantized features
print("\nEvaluating Quantized Model...")
quant_start_time = timeit.default_timer()
quant_loss, quant_acc, quant_auc = quantized_model.evaluate(features_quant, labels)
quant_final_time = timeit.default_timer()
quant_latency = quant_final_time - quant_start_time
quant_throughput = len(labels) / quant_latency
quant_model_size = os.path.getsize('../output/quantized_model.h5') / 1024.

# Compare original model performance with quantized model
print("\nSummarizing the performance between the orignal and quantized models:")
table.add_row(["Accuracy", format(orig_acc, '.2f'), format(quant_acc, '.2f')])
table.add_row(["AUC", format(orig_auc, '.2f'), format(quant_auc, '.2f')])
table.add_row(["Size (KB)", format(orig_model_size, '.2f'), format(quant_model_size, '.2f')])
table.add_row(["Latency (s)", format(orig_latency, '.2f'), format(quant_latency, '.2f')])
table.add_row(["Throughput", format(orig_throughput, '.2f'), format(quant_throughput,'.2f')])
print(table)

